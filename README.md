# ğŸŒ Multi-Site Scraper Project

## ğŸ› ï¸ Overview
This is a small project designed to scrape as many websites as you need with minimal developer effort. ğŸš€ The project structure is intentionally simple and efficient to streamline the creation of new scrapers. It's just a starting point, and you can customize it to fit your needs. ğŸŒŸ

---

## ğŸ—ï¸ Project Structure
The workflow is designed to minimize complexity and focus on daily tasks:

1ï¸âƒ£ **Index Page Handling**  
   - Obtain an index URL and work on it daily. A new day means a new index page to scrape. ğŸ“…  
   - Process a collection of data using tools like `BeautifulSoup` (bs4) or other scraping/crawling libraries. ğŸ•¸ï¸  

2ï¸âƒ£ **Field Selectors Setup**  
   - Define field selectors using XPath or BS4 selectors for each data field (e.g., `Bus` fields or other entities).  
   - Ensure detailed pages are processed effectively. âœ…  

---

## âš ï¸ Considerations
- Works best for websites with an **index page** and corresponding **detail pages**. ğŸ–±ï¸â¡ï¸ğŸ“„  
- The Bus model includes 3 scraped fields as an example. You can easily expand this by adding more fields as needed. ğŸ› ï¸  
- The scraper is designed to **run manually** in two stages:  
  1ï¸âƒ£ Scraping all index pages first to gather detail URLs.  
  2ï¸âƒ£ Scraping all detail pages to retrieve additional information.

---

ğŸ¯ **Start scraping smarter, not harder!** Happy coding! ğŸ’»âœ¨

---

## ğŸš€ Installation

1ï¸âƒ£ **Set up your virtual environment**  
   Generate a virtual environment to isolate project dependencies:
   ```bash
   python3 -m venv venv
   ```

2ï¸âƒ£ **Activate the virtual environment**  
   Activate it using the following command:
   ```bash
   source venv/bin/activate
   ```

3ï¸âƒ£ **Install dependencies**  
   Install all required libraries from `requirements.txt`:
   ```bash
   pip install -r requirements.txt
   ```
4ï¸âƒ£ **Setup Django**  
   Initialize the database and create a superuser:
   ```bash
   python manage.py migrate
   python manage.py createsuperuser
   ```


4ï¸âƒ£ **Run the scraper**  
   Use the following command to execute the scraper:
   ```bash
   python3 manage.py scrap
   ```

---
## ğŸš€ Visualizations

5ï¸âƒ£ **Run the server**
    ```bash
    python3 manage.py runserver
    ```
6ï¸âƒ£ **Open the browser and go to the following link**
    ```bash
    http://localhost:8000/admin
    ```

ğŸš€ ğŸš€ Now you can login with the superuser credentials, and review the Bus scrapped data ğŸš€ ğŸš€


ğŸŒŸ Enjoy building powerful scrapers with ease! If you encounter any issues or want to suggest improvements, feel free to contribute! ğŸš€âœ¨
